# 站点如何防止爬虫？【热度: 554】

- Issue: #734
- State: open
- Labels: web应用场景, 百度
- Author: yanlele
- URL: https://github.com/pro-collection/interview-question/issues/734
- Created: 2024-05-05T14:47:30Z
- Updated: 2024-05-05T14:47:30Z

## Body

**关键词**：反爬虫

站点防止爬虫通常涉及一系列技术和策略的组合。以下是一些常用的方法：

### 1. 修改 `robots.txt`

在站点的根目录下创建或修改 `robots.txt` 文件，用来告知遵守该协议的爬虫应该爬取哪些页面，哪些不应该爬取。例如：

```txt
User-agent: *
Disallow: /
```

然而，需要注意的是遵守 `robots.txt` 不是强制性的，恶意爬虫可以忽视这些规则。

### 2. 使用 CAPTCHA

对于表单提交、登录页面等，使用验证码（CAPTCHA）可以防止自动化脚本或机器人执行操作。

### 3. 检查用户代理字符串

服务器可以根据请求的用户代理（User-Agent）字符串来决定是否屏蔽某些爬虫。但用户代理字符串可以伪造，所以这不是一个完全可靠的方法。

### 4. 分析流量行为

分析访问者的行为，比如访问频率、访问页数、访问时长，并与正常用户的行为进行对比，从而尝试检测和屏蔽爬虫。

### 5. 使用 Web 应用防火墙（WAF）

许多 Web 应用防火墙提供自动化的爬虫和机器人检测功能，可以帮助防止爬虫。

### 6. 服务端渲染和动态 Token

一些网站使用 JavaScript 服务端渲染，或将关键内容（比如令牌）动态地插入到页面中，这可以使得非浏览器的自动化工具获取网站内容变得更加困难。

### 7. 添加额外的 HTTP 头

一些站点要求每个请求都包括特定的 HTTP 头，这些头信息不是常规爬虫会添加的，而是通过 JavaScript 动态添加的。

### 8. IP 黑名单

如果探测到某个 IP 地址的不正常行为，就可以将该 IP 地址加入黑名单，阻止其进一步的访问。

### 9. 限制访问速度

通过限制特定时间内允许的请求次数来禁止爬虫执行大量快速的页面抓取。

### 10. API 限流

对 API 使用率进行限制，比如基于用户、IP 地址等实施限速和配额。

### 11. 使用 HTTPS

使用 HTTPS 加密您的网站，这可以避免中间人攻击，并增加爬虫的抓取难度。

### 12. 更改网站结构和内容

定期更改网站的 URL 结构、内容排版等，使得爬虫开发人员需要不断更新爬虫程序来跟进网站的改动。

